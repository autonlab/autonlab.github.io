---
layout: default
first_name: Nick
last_name: Gisolfi
title: Project Scientist
category: staff
summary: "Project Scientist"
image: "/assets/staff/gisolfi_nick.jpeg"
---

# Nicholas Gisolfi

<img src="{{'/assets/staff/gisolfi_nick.jpeg' | relative_url}}" alt="Nick" style="width:30%;float:right">

I have been part of the Auton Lab since 2013 when I started my Masters at the RI working with [Artur Dubrawki][1] and mentored by [Ina Fiterau][2] who was a senior PhD student at the time.  I continued working with Artur for my PhD in 2015 and I graduated in December 2021.  I was so happy to be able to have Ina as a member of my thesis committee!

My dissertation focused on **Model-Centric Verification of Artificial Intelligence**.  In it, I lay out new methods for assessing whether an AI makes decisions in a similar way to how a trustworthy human would make their decision.  My goal with this work is to make AI more trustworthy and better suited to critical contexts, where it is important to provide assurances that AI will never inflict otherwise easily preventable harm to humans.  A longstanding and open question in machine learning is *what did the trained AI system actually learn from the data we gave it?* -- my work provides some answers to that question in application contexts that range from:

1. Radiation Safety - ensuring that AI 
2. Critical Care Medicine - proving that an AI adheres to a clinician's expert knowledge and common-sense expectations
3. Algorithmic Fairness - proving that an AI does not yield substantially different recommendations for two individuals who are virtually identical except for immutable characteristics such as race or gender

Research never really ends.  Where I hope to take this work in my future is to establish new mechanisms by which AI can become regulated by governing agencies.  I see an uncanny parallel between the history of the automobile industry and the current state of AI; in times before there were a set of criteria that determine what it means for a vehicle to be certified as road safe (seeat belts, breakaway steering, airbags, etc) a lot of easily preventable harm was done to motorists.  By enforcing safety measures for new vehicles, they become safer and more ubiquitous.  I see a future for similar types of innovation in AI, where it will be much easier to warrant using AI in new critical contexts if we can prove that the sytem we wish to deploy is safe.

Now I am in a new role as a Project Scientist in the lab!

### Manuscripts
My [PhD thesis][3]

### Repos
[Tree Ensemble Accreditor][4]


### Community
I am interested in [community building][5], which I interepret as fostering connections through shared experiences.  I enjoy organizing events that get people working towards common goals.

- [2018.HackAuton][6]
- [RI Robot Building Contest][7]

[1]:</staff/dubrawski_artur.html>
[2]:</staff/fiterau_madalina.html>
[3]:<https://www.ri.cmu.edu/publications/model-centric-verification-of-artificial-intelligence/>
[4]:<http://git.int.autonlab.org/ngisolfi/TEA>
[5]:<https://www.science.org/content/article/building-community-career>
[6]:<http://www.cs.cmu.edu/~ngisolfi/hackauton.html>
[7]:<http://www.cs.cmu.edu/~ngisolfi/orientation.html>

